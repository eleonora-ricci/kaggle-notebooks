{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/eleonoraricci/ps3e14-berrylicious-predictions-with-pytorch?scriptVersionId=128619927\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ba5bf898","metadata":{"papermill":{"duration":0.006466,"end_time":"2023-05-07T09:00:38.604007","exception":false,"start_time":"2023-05-07T09:00:38.597541","status":"completed"},"tags":[]},"source":["# **PyTorchðŸ”¥ model and FLAMLðŸ”¥ hyperparameter tuning**\n","\n","In this Notebook I wanted to share a **baseline Neural Network** implementation in **PyTorch** to model the *Wild Blueberry Yield Dataset* for the episode 14 of season 3 of the playGround competitions series on Kaggle.\n","\n","The architecture consists of a sequence of densely connected layers. **EarlyStopping** and **Dropout** are implemented and can be tinkered with by changing the corresponding parameters in the code.\n","_____________________________\n","\n","*--- Edit 23/05/06: implemented **automatic HP search with FLAML**. Main inspirations: **[this link](https://microsoft.github.io/FLAML/docs/Examples/Tune-PyTorch/)** and **[this notebook](https://www.kaggle.com/code/paddykb/ps-s3e14-flaml-bfi-be-bop-a-blueberry-do-dah/notebook)**. ---*\n","_____________________________\n","\n","**Preliminary observation:** just by scaling the features ([StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) **the MAE went down by ~50**.\n","_____________________________\n","Now off to some feature engineering, and some more hyperparameter tuning ðŸ‘‹\n","\n","Any feedback or question is more than welcome!! ðŸ§"]},{"cell_type":"code","execution_count":1,"id":"3620d057","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-05-07T09:00:38.61849Z","iopub.status.busy":"2023-05-07T09:00:38.618022Z","iopub.status.idle":"2023-05-07T09:00:53.722446Z","shell.execute_reply":"2023-05-07T09:00:53.720657Z"},"papermill":{"duration":15.115358,"end_time":"2023-05-07T09:00:53.725652","exception":false,"start_time":"2023-05-07T09:00:38.610294","status":"completed"},"tags":[]},"outputs":[],"source":["%%capture\n","!pip install flaml"]},{"cell_type":"code","execution_count":2,"id":"f217de94","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-07T09:00:53.742794Z","iopub.status.busy":"2023-05-07T09:00:53.741779Z","iopub.status.idle":"2023-05-07T09:01:04.477035Z","shell.execute_reply":"2023-05-07T09:01:04.475807Z"},"papermill":{"duration":10.747177,"end_time":"2023-05-07T09:01:04.479658","exception":false,"start_time":"2023-05-07T09:00:53.732481","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import time\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","from typing import List, Callable\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","torch.set_default_dtype(torch.float32)\n","\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer\n","\n","from ray import tune\n","import flaml\n","\n","seed = 17\n","torch.manual_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","id":"a908ccaa","metadata":{"papermill":{"duration":0.005448,"end_time":"2023-05-07T09:01:04.491058","exception":false,"start_time":"2023-05-07T09:01:04.48561","status":"completed"},"tags":[]},"source":["# **Neural network and auxiliary classes**"]},{"cell_type":"code","execution_count":3,"id":"697eab19","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.504283Z","iopub.status.busy":"2023-05-07T09:01:04.503925Z","iopub.status.idle":"2023-05-07T09:01:04.517721Z","shell.execute_reply":"2023-05-07T09:01:04.516363Z"},"papermill":{"duration":0.023546,"end_time":"2023-05-07T09:01:04.520146","exception":false,"start_time":"2023-05-07T09:01:04.4966","status":"completed"},"tags":[]},"outputs":[],"source":["class NN(nn.Module):\n","    \"\"\"Simple densely connected neural network model\"\"\"\n","    \n","    def __init__(self, n_hidden_layers: int, \n","                       input_dim: int, \n","                       hidden_dims: List[int], \n","                       output_dim:int, \n","                       dropout_prob: float,\n","                       activation: str):\n","        \n","        \"\"\"      \n","        Parameters\n","        ----------\n","        n_hidden_layers: int\n","            The number of hidden layers to include between input and output\n","\n","        input_dim: int\n","            Number of nodes in the input layer. It is equal to the number of \n","            features used for modelling.\n","            \n","        hidden_dims: List[int]\n","            List of ints specifying the number of nodes in each hidden layer \n","            of the network.\n","            \n","        output_dim: int\n","            Number of nodes in the output layer. It is equal to 1 for \n","            this regression task.\n","            \n","        dropout_prob: float\n","            value between 0 and 1: probability of zeroing out elements of the \n","            input tensor in the Dropout layer\n","            \n","        activation: str\n","            the name of the activation function from the torch.nn module is \n","            passed as a string and then resolved by getattr(nn, activation)()\n","\n","        \"\"\"\n","\n","        super(NN, self).__init__()\n","        \n","        # The Network is built using the ModuleList constructor        \n","        self.network = nn.ModuleList()\n","        \n","        if n_hidden_layers == 0:\n","            # If no hidden layers are used, go directly from input to \n","            # output dimension\n","            self.network.append(nn.Linear(input_dim, output_dim))\n","            self.network.append(getattr(nn, activation)())\n","        \n","        else:\n","            # Append to the constructor the required layers, with dimentions specified\n","            # in the hidden_dims list\n","            if dropout_prob > 0:\n","                self.network.append(torch.nn.Dropout(p=dropout_prob, inplace=True))\n","            self.network.append(nn.Linear(input_dim, hidden_dims[0]))\n","            self.network.append(nn.LayerNorm(hidden_dims[0]))\n","            self.network.append(getattr(nn, activation)())\n","\n","            for layer in range(n_hidden_layers-1):\n","                self.network.append(nn.Linear(hidden_dims[layer], hidden_dims[layer+1]))\n","                self.network.append(nn.LayerNorm(hidden_dims[layer+1]))\n","                self.network.append(getattr(nn, activation)())\n","\n","            self.network.append(nn.Linear(hidden_dims[-1], output_dim))\n","            self.network.append(nn.ReLU())\n","    \n","    def forward(self, x):\n","        for layer in self.network:\n","            x = layer(x)  \n","            \n","        return x\n","    \n","\n","def get_model(n_features: int, \n","              hidden_dims: List[int], \n","              activation: str, \n","              dropout_prob: float):\n","    \n","    \"\"\"Return a newly initialized NN model to perform HP tuning rounds\n","    \n","    Parameters\n","    ----------\n","            \n","    hidden_dims: List[int]\n","        List of ints specifying the number of nodes in each hidden layer \n","        of the network. e.g. hidden_dims = [100, 100, 100]\n","        \n","    activation: str\n","        the name of the activation function from the torch.nn module is \n","        passed as a string and then resolved by getattr(nn, activation)()\n","        \n","    dropout_prob: float\n","        value between 0 and 1: probability of zeroing out elements of the \n","        input tensor in the Dropout layer\n","    \"\"\" \n","    \n","    model = NN(n_hidden_layers = len(hidden_dims), \n","               input_dim = n_features, \n","               hidden_dims = hidden_dims, \n","               output_dim = 1, \n","               activation = activation, \n","               dropout_prob = dropout_prob)\n","    \n","    return model"]},{"cell_type":"code","execution_count":4,"id":"e9ad40b3","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.535663Z","iopub.status.busy":"2023-05-07T09:01:04.535148Z","iopub.status.idle":"2023-05-07T09:01:04.547246Z","shell.execute_reply":"2023-05-07T09:01:04.545915Z"},"papermill":{"duration":0.024754,"end_time":"2023-05-07T09:01:04.551038","exception":false,"start_time":"2023-05-07T09:01:04.526284","status":"completed"},"tags":[]},"outputs":[],"source":["class EarlyStopping():\n","    \"\"\"\n","    Early stopping to stop the training when the validation loss does not improve after\n","    a certain number of epochs.\n","    \"\"\"\n","    def __init__(self, patience=5, min_delta=0.1):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        \n","        patience: how many epochs to wait before stopping when loss is not improving\n","               \n","        min_delta: minimum difference between new loss and old loss for new loss to \n","               be considered as an improvement\n","        \"\"\"\n","        \n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","        \n","    def __call__(self, val_loss):\n","        if self.best_loss == None:\n","            self.best_loss = val_loss\n","            \n","        elif self.best_loss - val_loss > self.min_delta:\n","            self.best_loss = val_loss\n","            # reset counter if validation loss improves\n","            self.counter = 0\n","            \n","        elif self.best_loss - val_loss < self.min_delta:\n","            self.counter += 1\n","            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n","            \n","            if self.counter >= self.patience:\n","                print('INFO: Early stopping')\n","                self.early_stop = True"]},{"cell_type":"code","execution_count":5,"id":"3af2e564","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.569986Z","iopub.status.busy":"2023-05-07T09:01:04.569402Z","iopub.status.idle":"2023-05-07T09:01:04.578275Z","shell.execute_reply":"2023-05-07T09:01:04.57639Z"},"papermill":{"duration":0.022611,"end_time":"2023-05-07T09:01:04.581931","exception":false,"start_time":"2023-05-07T09:01:04.55932","status":"completed"},"tags":[]},"outputs":[],"source":["class MyDataset(Dataset):\n","    \"\"\"A Custom Dataset class to facilitate batch iteration in PyTorch models\"\"\"\n","    def __init__(self, \n","                 features, \n","                 labels):\n","\n","        self.features = features\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","\n","        label = self.labels[idx]\n","        features = self.features[idx,:]\n","\n","        return features, label"]},{"cell_type":"code","execution_count":6,"id":"fb12cced","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.596514Z","iopub.status.busy":"2023-05-07T09:01:04.596072Z","iopub.status.idle":"2023-05-07T09:01:04.603007Z","shell.execute_reply":"2023-05-07T09:01:04.601473Z"},"papermill":{"duration":0.017011,"end_time":"2023-05-07T09:01:04.605383","exception":false,"start_time":"2023-05-07T09:01:04.588372","status":"completed"},"tags":[]},"outputs":[],"source":["class ColumnsDropper(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns=None):\n","        super(ColumnsDropper, self).__init__()\n","        self.columns=columns\n","\n","    def transform(self,X, y=None):\n","        return X.drop(self.columns,axis=1)\n","\n","    def fit(self, X, y=None):\n","        return self"]},{"cell_type":"markdown","id":"f62990cc","metadata":{"papermill":{"duration":0.00628,"end_time":"2023-05-07T09:01:04.618272","exception":false,"start_time":"2023-05-07T09:01:04.611992","status":"completed"},"tags":[]},"source":["# **Preprocessing pipeline**"]},{"cell_type":"code","execution_count":7,"id":"2cae73cc","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.634603Z","iopub.status.busy":"2023-05-07T09:01:04.634097Z","iopub.status.idle":"2023-05-07T09:01:04.767707Z","shell.execute_reply":"2023-05-07T09:01:04.766545Z"},"papermill":{"duration":0.145824,"end_time":"2023-05-07T09:01:04.770805","exception":false,"start_time":"2023-05-07T09:01:04.624981","status":"completed"},"tags":[]},"outputs":[],"source":["training_set = pd.read_csv(r'../input/playground-series-s3e14/train.csv')\n","orig_train = pd.read_csv(r'../input/wild-blueberry-yield-prediction-dataset/WildBlueberryPollinationSimulationData.csv')\n","orig_train.drop('Row#', axis = 1, inplace = True)\n","\n","training_set.drop('id', axis = 1, inplace = True)\n","combined_train = pd.concat([training_set, orig_train])\n","X = combined_train.copy()\n","\n","y = X.pop('yield')"]},{"cell_type":"code","execution_count":8,"id":"777079da","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.786164Z","iopub.status.busy":"2023-05-07T09:01:04.785713Z","iopub.status.idle":"2023-05-07T09:01:04.791935Z","shell.execute_reply":"2023-05-07T09:01:04.790599Z"},"papermill":{"duration":0.01694,"end_time":"2023-05-07T09:01:04.794511","exception":false,"start_time":"2023-05-07T09:01:04.777571","status":"completed"},"tags":[]},"outputs":[],"source":["# Minimal preprocessing, appling standard scaling to all features\n","columns_to_drop = ['MaxOfUpperTRange', 'MinOfUpperTRange', 'MaxOfLowerTRange', 'MinOfLowerTRange', 'RainingDays']\n","all_features = list(X.columns)\n","standard_scale = [feat for feat in all_features if feat not in columns_to_drop]"]},{"cell_type":"code","execution_count":9,"id":"23b6c196","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.808503Z","iopub.status.busy":"2023-05-07T09:01:04.808066Z","iopub.status.idle":"2023-05-07T09:01:04.835375Z","shell.execute_reply":"2023-05-07T09:01:04.833675Z"},"papermill":{"duration":0.038596,"end_time":"2023-05-07T09:01:04.839169","exception":false,"start_time":"2023-05-07T09:01:04.800573","status":"completed"},"tags":[]},"outputs":[],"source":["col_transformers=[\n","    ('std', StandardScaler(), standard_scale),\n","    # More transformers can be added as needed. \n","    # In ColumnTransformer one can specify different columns \n","    # lists for each preprocessing action to perform. \n","]\n","\n","preprocessor = ColumnTransformer(\n","    transformers=col_transformers,      \n","    remainder='passthrough')\n","\n","preproc_pipeline = Pipeline(steps=[('dropper', ColumnsDropper(columns = columns_to_drop)),\n","                                   ('preprocessor', preprocessor)])\n","\n","X_proc = preproc_pipeline.fit_transform(X)  "]},{"cell_type":"code","execution_count":10,"id":"deb2e379","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.857135Z","iopub.status.busy":"2023-05-07T09:01:04.856663Z","iopub.status.idle":"2023-05-07T09:01:04.866398Z","shell.execute_reply":"2023-05-07T09:01:04.864951Z"},"papermill":{"duration":0.023665,"end_time":"2023-05-07T09:01:04.870739","exception":false,"start_time":"2023-05-07T09:01:04.847074","status":"completed"},"tags":[]},"outputs":[],"source":["test_size = 0.2 \n","\n","# Splitting into training and validation set\n","X_train, X_val, y_train, y_val = train_test_split(X_proc, y.to_numpy(), test_size = test_size, random_state = seed)"]},{"cell_type":"markdown","id":"b43b1872","metadata":{"papermill":{"duration":0.006003,"end_time":"2023-05-07T09:01:04.883134","exception":false,"start_time":"2023-05-07T09:01:04.877131","status":"completed"},"tags":[]},"source":["# **Hyperparameter search space**"]},{"cell_type":"code","execution_count":11,"id":"d59b1fe9","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.898069Z","iopub.status.busy":"2023-05-07T09:01:04.897609Z","iopub.status.idle":"2023-05-07T09:01:04.905578Z","shell.execute_reply":"2023-05-07T09:01:04.904416Z"},"papermill":{"duration":0.018627,"end_time":"2023-05-07T09:01:04.90815","exception":false,"start_time":"2023-05-07T09:01:04.889523","status":"completed"},"tags":[]},"outputs":[],"source":["# Search space for HP tuning. \n","# This is a small set, to allow for the completion of a few tests within a short \n","# execution time span, to troubleshoot. It can be adjusted as needed for production.\n","\n","config = {\n","    \"layer1\": tune.choice([200]),\n","    \"layer2\": tune.choice([200]),\n","    \"layer3\": tune.choice([200]),  \n","    \"lr\": tune.choice([0.01]),\n","    \"batch_size\": tune.choice([100]),\n","    \"activation\": tune.choice([\"ReLU\", \"Tanh\"]),\n","    \"dropout_prob\": tune.choice([0, 0.2]), \n","}\n","\n","# Budget and resource constraints\n","time_budget_s = 600     # time budget in seconds\n","num_samples = 100       # maximal number of trials\n","\n","# Early stopping parameters\n","patience = 20\n","min_change = 0.0001\n"]},{"cell_type":"markdown","id":"26d769f6","metadata":{"papermill":{"duration":0.006233,"end_time":"2023-05-07T09:01:04.920638","exception":false,"start_time":"2023-05-07T09:01:04.914405","status":"completed"},"tags":[]},"source":["# **Training loop**"]},{"cell_type":"code","execution_count":12,"id":"54e0473c","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:01:04.936114Z","iopub.status.busy":"2023-05-07T09:01:04.935389Z","iopub.status.idle":"2023-05-07T09:01:04.950393Z","shell.execute_reply":"2023-05-07T09:01:04.949438Z"},"papermill":{"duration":0.02603,"end_time":"2023-05-07T09:01:04.953259","exception":false,"start_time":"2023-05-07T09:01:04.927229","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"source":["# The training loop is wrapped inside a function, to be able to pass \n","# this fuction as an argument for HP tuning.\n","\n","def train_nn(config):\n","    \n","    # Input size of the network equal to the number of retained feature (here all features)\n","    n_features = X_train.shape[1]\n","    \n","    # Creating Datasets and DataLoaders for batch iteration\n","    training_dataset = MyDataset(X_train, y_train)\n","    val_dataset = MyDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(training_dataset, batch_size=config[\"batch_size\"], shuffle=True) \n","    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n","    \n","    hidden_dims = [config[\"layer1\"], config[\"layer2\"], config[\"layer3\"]]\n","    model = get_model(n_features, hidden_dims, config[\"activation\"], config[\"dropout_prob\"])\n","    \n","    # MAE loss in PyTorch\n","    loss_function = torch.nn.L1Loss()\n","    \n","    # Initialization of the losses, instantiation of optimizer, early_stopper and learning rate scheduler\n","    loss_epoch = torch.tensor(0.0)\n","    loss_validation_epoch = torch.tensor(0.0)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n","\n","    early_stopper = EarlyStopping(patience = patience, min_delta = min_change)\n","\n","    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n","                    optimizer = optimizer, mode='min', patience = 10,\n","                    factor = 0.1, min_lr=1e-5, verbose=True)\n","                    \n","    # A high number of epochs can be used, since ealry stopping can interrupt the training if needed\n","    n_epochs = 500\n","\n","    for epoch in range(n_epochs):\n","\n","            # (re)initialize the tensor to accumulate batch loss values\n","            loss_batch = torch.tensor(0.0)\n","\n","            for i, data in enumerate(train_loader):\n","                # Every 'data' instance returned by the train_loader is an input + label pair\n","                inputs, labels = data\n","\n","                # Zero gradients for every batch\n","                optimizer.zero_grad()\n","\n","                # Make predictions for the current batch\n","                outputs = model(inputs.type(torch.float32))\n","\n","                # Compute the loss and perform back propagation\n","                loss = loss_function(outputs.squeeze(), labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Accumulate the loss over the batches\n","                loss_batch += loss.item()\n","\n","            # Average the loss over the number of batches\n","            loss_epoch = loss_batch / i\n","\n","            # (re)initialize the tensor to accumulate loss values for the validation set\n","            loss_validation_batch = torch.tensor(0.0)\n","\n","            for i, data in enumerate(val_loader):\n","                # Make prediction for each batch in the test set\n","                inputs, labels = data\n","                outputs_val = model(inputs.type(torch.float32))\n","\n","                # Accumulate the loss over the test set batches\n","                loss_val = loss_function(outputs_val.squeeze(), labels)\n","                loss_validation_batch += loss_val.item()\n","\n","            # Average the loss over the number of batches\n","            loss_validation_epoch = loss_validation_batch / i\n","\n","            print(\"Epoch: \", epoch +1 , \" - Train Loss: \", round(loss_epoch.item(), 4), \" - Val Loss: \", round(loss_validation_epoch.item(),4))\n","\n","            # Here we save a checkpoint. It is automatically registered with\n","            # Ray Tune and will potentially be passed as the `checkpoint_dir`\n","            # parameter in future iterations.\n","            with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n","                path = os.path.join(checkpoint_dir, \"checkpoint\")\n","                torch.save(\n","                    (model.state_dict(), optimizer.state_dict()), path)\n","\n","            tune.report(loss=loss_validation_epoch.item())\n","            # Update learning rate scheduler and check early stoopping criterun\n","            lr_scheduler.step(loss_validation_epoch)\n","\n","            early_stopper(loss_validation_epoch)\n","            if early_stopper.early_stop:\n","                break\n","            "]},{"cell_type":"markdown","id":"8dcbe695","metadata":{"papermill":{"duration":0.006395,"end_time":"2023-05-07T09:01:04.966176","exception":false,"start_time":"2023-05-07T09:01:04.959781","status":"completed"},"tags":[]},"source":["# **HP optimization rounds**"]},{"cell_type":"code","execution_count":13,"id":"a47a6fe0","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-05-07T09:01:04.982137Z","iopub.status.busy":"2023-05-07T09:01:04.981125Z","iopub.status.idle":"2023-05-07T09:15:34.796975Z","shell.execute_reply":"2023-05-07T09:15:34.795253Z"},"papermill":{"duration":869.827327,"end_time":"2023-05-07T09:15:34.799989","exception":false,"start_time":"2023-05-07T09:01:04.972662","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2023-05-07 09:01:04,988]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n","2023-05-07 09:01:09,493\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"]},{"data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2023-05-07 09:15:34</td></tr>\n","<tr><td>Running for: </td><td>00:14:22.51        </td></tr>\n","<tr><td>Memory:      </td><td>1.8/31.4 GiB       </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/17.62 GiB heap, 0.0/8.81 GiB objects\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name       </th><th>status    </th><th>loc           </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_prob</th><th style=\"text-align: right;\">  layer1</th><th style=\"text-align: right;\">  layer2</th><th style=\"text-align: right;\">  layer3</th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_nn_55cbb65b</td><td>TERMINATED</td><td>172.19.2.2:424</td><td>Tanh        </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           0  </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         71.9932</td><td style=\"text-align: right;\">368.056</td></tr>\n","<tr><td>train_nn_47b1c36d</td><td>TERMINATED</td><td>172.19.2.2:424</td><td>ReLU        </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         51.6671</td><td style=\"text-align: right;\">385.613</td></tr>\n","<tr><td>train_nn_9f24a79e</td><td>TERMINATED</td><td>172.19.2.2:424</td><td>ReLU        </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           0  </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         61.8779</td><td style=\"text-align: right;\">361.027</td></tr>\n","<tr><td>train_nn_8adc96d7</td><td>TERMINATED</td><td>172.19.2.2:424</td><td>Tanh        </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         68.8186</td><td style=\"text-align: right;\">388.712</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div class=\"trialProgress\">\n","  <h3>Trial Progress</h3>\n","  <table>\n","<thead>\n","<tr><th>Trial name       </th><th style=\"text-align: right;\">   loss</th><th>should_checkpoint  </th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_nn_47b1c36d</td><td style=\"text-align: right;\">385.613</td><td>True               </td></tr>\n","<tr><td>train_nn_55cbb65b</td><td style=\"text-align: right;\">368.056</td><td>True               </td></tr>\n","<tr><td>train_nn_8adc96d7</td><td style=\"text-align: right;\">388.712</td><td>True               </td></tr>\n","<tr><td>train_nn_9f24a79e</td><td style=\"text-align: right;\">361.027</td><td>True               </td></tr>\n","</tbody>\n","</table>\n","</div>\n","<style>\n",".trialProgress {\n","  display: flex;\n","  flex-direction: column;\n","  color: var(--jp-ui-font-color1);\n","}\n",".trialProgress h3 {\n","  font-weight: bold;\n","}\n",".trialProgress td {\n","  white-space: nowrap;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  1  - Train Loss:  5932.3955  - Val Loss:  5943.0347\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  2  - Train Loss:  5652.9028  - Val Loss:  5663.5664\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  3  - Train Loss:  5384.3325  - Val Loss:  5393.2368\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  4  - Train Loss:  5119.8896  - Val Loss:  5119.3677\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  5  - Train Loss:  4855.2822  - Val Loss:  4873.5391\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  6  - Train Loss:  4593.0088  - Val Loss:  4579.7681\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  7  - Train Loss:  4329.7129  - Val Loss:  4312.9668\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  8  - Train Loss:  4065.3911  - Val Loss:  4043.5637\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  9  - Train Loss:  3804.9927  - Val Loss:  3772.7954\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  10  - Train Loss:  3545.9878  - Val Loss:  3523.8188\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  11  - Train Loss:  3289.1509  - Val Loss:  3266.7354\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  12  - Train Loss:  3037.2075  - Val Loss:  3015.0977\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  13  - Train Loss:  2789.282  - Val Loss:  2758.7314\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  14  - Train Loss:  2551.3926  - Val Loss:  2488.8848\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  15  - Train Loss:  2331.7034  - Val Loss:  2289.7139\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  16  - Train Loss:  2127.6077  - Val Loss:  2095.5288\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  17  - Train Loss:  1946.8125  - Val Loss:  1931.1479\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  18  - Train Loss:  1790.2012  - Val Loss:  1779.5548\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  19  - Train Loss:  1653.8158  - Val Loss:  1652.8685\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  20  - Train Loss:  1533.5593  - Val Loss:  1534.4673\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  21  - Train Loss:  1430.3051  - Val Loss:  1424.1309\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  22  - Train Loss:  1356.8998  - Val Loss:  1360.588\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  23  - Train Loss:  1271.3689  - Val Loss:  1294.7236\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  24  - Train Loss:  1214.5712  - Val Loss:  1240.6462\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  25  - Train Loss:  1172.9059  - Val Loss:  1203.4506\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  26  - Train Loss:  1143.6713  - Val Loss:  1178.6102\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  27  - Train Loss:  1125.8472  - Val Loss:  1152.8186\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  28  - Train Loss:  1114.1439  - Val Loss:  1154.4785\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  29  - Train Loss:  1097.2645  - Val Loss:  1114.9146\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  30  - Train Loss:  875.1291  - Val Loss:  762.3886\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  31  - Train Loss:  654.758  - Val Loss:  646.9646\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  32  - Train Loss:  586.3851  - Val Loss:  586.2453\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  33  - Train Loss:  533.8799  - Val Loss:  541.4074\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  34  - Train Loss:  496.8043  - Val Loss:  520.7219\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  35  - Train Loss:  468.8986  - Val Loss:  492.5703\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  36  - Train Loss:  447.3587  - Val Loss:  475.4103\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  37  - Train Loss:  426.2869  - Val Loss:  454.0528\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  38  - Train Loss:  408.8362  - Val Loss:  425.4865\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  39  - Train Loss:  395.7632  - Val Loss:  422.493\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  40  - Train Loss:  387.7779  - Val Loss:  409.3414\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  41  - Train Loss:  382.4376  - Val Loss:  403.9259\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  42  - Train Loss:  375.565  - Val Loss:  405.2355\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  43  - Train Loss:  371.1359  - Val Loss:  401.1117\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  44  - Train Loss:  365.0621  - Val Loss:  385.6067\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  45  - Train Loss:  362.5165  - Val Loss:  393.1027\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  46  - Train Loss:  361.0548  - Val Loss:  391.8954\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  47  - Train Loss:  357.7888  - Val Loss:  386.9569\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  48  - Train Loss:  355.1654  - Val Loss:  384.8134\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  49  - Train Loss:  354.1658  - Val Loss:  385.1968\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  50  - Train Loss:  352.9142  - Val Loss:  377.5594\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  51  - Train Loss:  349.2999  - Val Loss:  382.2101\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  52  - Train Loss:  349.1042  - Val Loss:  384.3525\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  53  - Train Loss:  346.7778  - Val Loss:  381.4627\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  54  - Train Loss:  347.9192  - Val Loss:  380.9439\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  55  - Train Loss:  346.7785  - Val Loss:  375.9242\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  56  - Train Loss:  343.7812  - Val Loss:  372.4755\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  57  - Train Loss:  343.1731  - Val Loss:  367.2303\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  58  - Train Loss:  340.4518  - Val Loss:  368.9315\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  59  - Train Loss:  340.2787  - Val Loss:  376.436\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  60  - Train Loss:  339.9528  - Val Loss:  372.6492\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  61  - Train Loss:  339.3505  - Val Loss:  370.4687\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  62  - Train Loss:  338.3982  - Val Loss:  371.5052\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  63  - Train Loss:  337.9859  - Val Loss:  370.0716\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  64  - Train Loss:  336.4886  - Val Loss:  366.9413\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  65  - Train Loss:  335.5605  - Val Loss:  375.8982\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  66  - Train Loss:  336.1797  - Val Loss:  371.5564\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  67  - Train Loss:  335.0085  - Val Loss:  366.8621\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  68  - Train Loss:  334.68  - Val Loss:  374.422\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  69  - Train Loss:  333.8892  - Val Loss:  367.1498\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  70  - Train Loss:  334.8266  - Val Loss:  372.2168\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  71  - Train Loss:  332.3476  - Val Loss:  367.9455\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  72  - Train Loss:  332.1782  - Val Loss:  364.7643\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  73  - Train Loss:  333.4716  - Val Loss:  371.8049\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  74  - Train Loss:  331.9095  - Val Loss:  374.397\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  75  - Train Loss:  331.1237  - Val Loss:  366.9338\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  76  - Train Loss:  330.9335  - Val Loss:  374.0317\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  77  - Train Loss:  329.8582  - Val Loss:  374.0818\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  78  - Train Loss:  330.2271  - Val Loss:  378.2473\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  79  - Train Loss:  329.972  - Val Loss:  368.972\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  80  - Train Loss:  328.749  - Val Loss:  369.3503\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  81  - Train Loss:  329.4345  - Val Loss:  372.1818\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  82  - Train Loss:  327.3069  - Val Loss:  372.378\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  83  - Train Loss:  327.2887  - Val Loss:  371.4972\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00083: reducing learning rate of group 0 to 1.0000e-03.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  84  - Train Loss:  321.3869  - Val Loss:  364.8636\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 12 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  85  - Train Loss:  318.8945  - Val Loss:  367.2068\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 13 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  86  - Train Loss:  318.274  - Val Loss:  366.9066\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 14 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  87  - Train Loss:  318.4619  - Val Loss:  369.6309\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 15 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  88  - Train Loss:  317.7778  - Val Loss:  365.1433\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 16 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  89  - Train Loss:  318.1186  - Val Loss:  365.5384\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 17 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  90  - Train Loss:  317.4141  - Val Loss:  373.1131\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 18 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  91  - Train Loss:  317.3435  - Val Loss:  368.3827\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 19 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  92  - Train Loss:  317.3065  - Val Loss:  368.0555\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 20 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  1  - Train Loss:  5869.5469  - Val Loss:  5558.6592\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  2  - Train Loss:  4447.3647  - Val Loss:  3309.6497\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  3  - Train Loss:  1886.8773  - Val Loss:  902.6578\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  4  - Train Loss:  596.8999  - Val Loss:  508.3782\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  5  - Train Loss:  456.1545  - Val Loss:  455.1068\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  6  - Train Loss:  424.8983  - Val Loss:  430.1014\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  7  - Train Loss:  408.8834  - Val Loss:  426.6214\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  8  - Train Loss:  403.1461  - Val Loss:  415.6121\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  9  - Train Loss:  391.2822  - Val Loss:  417.0119\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  10  - Train Loss:  392.2819  - Val Loss:  415.7629\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  11  - Train Loss:  390.8394  - Val Loss:  416.9169\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  12  - Train Loss:  387.3662  - Val Loss:  405.8825\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  13  - Train Loss:  387.9534  - Val Loss:  411.7171\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  14  - Train Loss:  386.3064  - Val Loss:  396.6649\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  15  - Train Loss:  382.7001  - Val Loss:  401.968\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  16  - Train Loss:  383.41  - Val Loss:  395.1845\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  17  - Train Loss:  380.1277  - Val Loss:  406.2295\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  18  - Train Loss:  381.5303  - Val Loss:  399.6143\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  19  - Train Loss:  379.2826  - Val Loss:  408.8986\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  20  - Train Loss:  379.0071  - Val Loss:  395.6508\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  21  - Train Loss:  378.4779  - Val Loss:  401.6431\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  22  - Train Loss:  380.1282  - Val Loss:  400.1735\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  23  - Train Loss:  381.5718  - Val Loss:  395.6477\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  24  - Train Loss:  380.9932  - Val Loss:  407.3081\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  25  - Train Loss:  384.066  - Val Loss:  410.015\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  26  - Train Loss:  377.5066  - Val Loss:  385.4798\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  27  - Train Loss:  375.7769  - Val Loss:  390.981\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  28  - Train Loss:  379.1614  - Val Loss:  401.3703\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  29  - Train Loss:  375.9714  - Val Loss:  399.184\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  30  - Train Loss:  376.4412  - Val Loss:  403.0877\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  31  - Train Loss:  374.7755  - Val Loss:  395.1021\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  32  - Train Loss:  373.3641  - Val Loss:  389.3232\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  33  - Train Loss:  378.269  - Val Loss:  398.0288\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  34  - Train Loss:  377.554  - Val Loss:  391.2265\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  35  - Train Loss:  375.6511  - Val Loss:  405.2053\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  36  - Train Loss:  376.916  - Val Loss:  399.0209\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  37  - Train Loss:  376.8769  - Val Loss:  395.3239\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00037: reducing learning rate of group 0 to 1.0000e-03.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  38  - Train Loss:  367.5051  - Val Loss:  382.5091\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  39  - Train Loss:  368.8302  - Val Loss:  389.4571\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  40  - Train Loss:  366.4207  - Val Loss:  389.9423\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  41  - Train Loss:  365.1363  - Val Loss:  385.1006\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  42  - Train Loss:  367.2999  - Val Loss:  385.2478\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  43  - Train Loss:  364.1191  - Val Loss:  391.4979\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  44  - Train Loss:  365.5226  - Val Loss:  394.3275\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  45  - Train Loss:  368.3327  - Val Loss:  389.1227\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  46  - Train Loss:  364.1353  - Val Loss:  387.7171\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  47  - Train Loss:  366.4707  - Val Loss:  389.6599\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  48  - Train Loss:  365.2378  - Val Loss:  385.756\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  49  - Train Loss:  366.7144  - Val Loss:  390.5381\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  50  - Train Loss:  360.431  - Val Loss:  384.961\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 12 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  51  - Train Loss:  362.8568  - Val Loss:  385.2328\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 13 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  52  - Train Loss:  364.1109  - Val Loss:  398.8718\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 14 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  53  - Train Loss:  362.0018  - Val Loss:  384.4203\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 15 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  54  - Train Loss:  361.2636  - Val Loss:  384.6931\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 16 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  55  - Train Loss:  363.6651  - Val Loss:  387.8247\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 17 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  56  - Train Loss:  364.5382  - Val Loss:  385.3456\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 18 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  57  - Train Loss:  362.4382  - Val Loss:  389.2765\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 19 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  58  - Train Loss:  362.6451  - Val Loss:  385.6134\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 20 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  1  - Train Loss:  5821.6562  - Val Loss:  5494.1611\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  2  - Train Loss:  4334.9404  - Val Loss:  3128.3901\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  3  - Train Loss:  1734.8  - Val Loss:  809.1999\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  4  - Train Loss:  507.2945  - Val Loss:  427.1615\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  5  - Train Loss:  383.2703  - Val Loss:  391.5023\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  6  - Train Loss:  366.1931  - Val Loss:  381.7706\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  7  - Train Loss:  361.7562  - Val Loss:  387.3261\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  8  - Train Loss:  357.5896  - Val Loss:  376.264\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  9  - Train Loss:  350.6645  - Val Loss:  377.4982\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  10  - Train Loss:  348.3607  - Val Loss:  366.8596\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  11  - Train Loss:  348.9616  - Val Loss:  377.4896\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  12  - Train Loss:  348.595  - Val Loss:  373.0687\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  13  - Train Loss:  350.2625  - Val Loss:  372.8649\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  14  - Train Loss:  346.6328  - Val Loss:  366.9123\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  15  - Train Loss:  346.6845  - Val Loss:  387.3059\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  16  - Train Loss:  345.0605  - Val Loss:  376.1337\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  17  - Train Loss:  345.6283  - Val Loss:  369.585\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  18  - Train Loss:  344.5556  - Val Loss:  362.7467\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  19  - Train Loss:  343.1335  - Val Loss:  366.2084\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  20  - Train Loss:  342.5642  - Val Loss:  366.6457\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  21  - Train Loss:  341.8879  - Val Loss:  370.2985\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  22  - Train Loss:  344.7995  - Val Loss:  368.2872\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  23  - Train Loss:  343.4572  - Val Loss:  366.6931\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  24  - Train Loss:  339.984  - Val Loss:  371.2702\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  25  - Train Loss:  341.3461  - Val Loss:  367.0282\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  26  - Train Loss:  341.3844  - Val Loss:  363.4048\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  27  - Train Loss:  343.1658  - Val Loss:  364.7011\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  28  - Train Loss:  340.0168  - Val Loss:  378.5017\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  29  - Train Loss:  339.7245  - Val Loss:  369.4622\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00029: reducing learning rate of group 0 to 1.0000e-03.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  30  - Train Loss:  332.9038  - Val Loss:  362.2942\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  31  - Train Loss:  330.718  - Val Loss:  362.2724\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  32  - Train Loss:  331.0175  - Val Loss:  358.7299\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  33  - Train Loss:  330.5281  - Val Loss:  366.0342\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  34  - Train Loss:  330.0777  - Val Loss:  362.5182\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  35  - Train Loss:  330.0285  - Val Loss:  365.3578\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  36  - Train Loss:  329.9645  - Val Loss:  362.1341\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  37  - Train Loss:  329.7729  - Val Loss:  366.2398\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  38  - Train Loss:  330.2493  - Val Loss:  364.4935\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  39  - Train Loss:  330.0739  - Val Loss:  369.0948\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  40  - Train Loss:  329.2789  - Val Loss:  372.1499\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  41  - Train Loss:  329.3102  - Val Loss:  363.269\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  42  - Train Loss:  329.0964  - Val Loss:  360.7114\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  43  - Train Loss:  329.7776  - Val Loss:  363.4235\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  44  - Train Loss:  328.2725  - Val Loss:  359.8201\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 12 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  45  - Train Loss:  327.7039  - Val Loss:  362.0361\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 13 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  46  - Train Loss:  327.8154  - Val Loss:  364.4655\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 14 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  47  - Train Loss:  327.9346  - Val Loss:  357.8812\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  48  - Train Loss:  327.8011  - Val Loss:  362.0292\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  49  - Train Loss:  327.9164  - Val Loss:  365.322\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  50  - Train Loss:  328.0386  - Val Loss:  357.6979\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  51  - Train Loss:  328.4015  - Val Loss:  363.0578\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  52  - Train Loss:  327.6576  - Val Loss:  365.7824\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  53  - Train Loss:  328.1738  - Val Loss:  362.5431\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  54  - Train Loss:  327.7296  - Val Loss:  358.8994\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  55  - Train Loss:  327.8897  - Val Loss:  364.7447\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  56  - Train Loss:  328.137  - Val Loss:  360.4255\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  57  - Train Loss:  328.1438  - Val Loss:  362.7364\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  58  - Train Loss:  327.8555  - Val Loss:  361.7792\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  59  - Train Loss:  327.6559  - Val Loss:  360.2313\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  60  - Train Loss:  327.6703  - Val Loss:  362.4395\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  61  - Train Loss:  327.6722  - Val Loss:  360.0822\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  62  - Train Loss:  328.0471  - Val Loss:  359.8835\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 12 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  63  - Train Loss:  327.6851  - Val Loss:  359.8436\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 13 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  64  - Train Loss:  327.7135  - Val Loss:  360.8387\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 14 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  65  - Train Loss:  327.7375  - Val Loss:  362.3932\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 15 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  66  - Train Loss:  327.6104  - Val Loss:  362.31\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 16 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  67  - Train Loss:  327.9293  - Val Loss:  359.0088\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 17 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  68  - Train Loss:  327.8973  - Val Loss:  363.9035\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 18 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  69  - Train Loss:  327.784  - Val Loss:  360.6582\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 19 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  70  - Train Loss:  327.5079  - Val Loss:  361.0273\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 20 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  1  - Train Loss:  5929.813  - Val Loss:  5945.5928\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  2  - Train Loss:  5656.4814  - Val Loss:  5680.1777\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  3  - Train Loss:  5385.7856  - Val Loss:  5380.1943\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  4  - Train Loss:  5120.7495  - Val Loss:  5119.7949\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  5  - Train Loss:  4856.9419  - Val Loss:  4879.7856\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  6  - Train Loss:  4593.6548  - Val Loss:  4557.6777\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  7  - Train Loss:  4331.7446  - Val Loss:  4332.8252\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  8  - Train Loss:  4066.8872  - Val Loss:  4036.3992\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  9  - Train Loss:  3808.4453  - Val Loss:  3771.7166\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  10  - Train Loss:  3547.8914  - Val Loss:  3523.7163\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  11  - Train Loss:  3291.8652  - Val Loss:  3245.4387\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  12  - Train Loss:  3041.9546  - Val Loss:  3010.7419\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  13  - Train Loss:  2793.1062  - Val Loss:  2756.0181\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  14  - Train Loss:  2557.8999  - Val Loss:  2529.968\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  15  - Train Loss:  2334.4922  - Val Loss:  2300.4348\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  16  - Train Loss:  2129.7231  - Val Loss:  2094.5425\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  17  - Train Loss:  1949.1426  - Val Loss:  1937.3076\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  18  - Train Loss:  1793.2363  - Val Loss:  1778.9691\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  19  - Train Loss:  1657.5308  - Val Loss:  1648.4473\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  20  - Train Loss:  1536.0856  - Val Loss:  1523.2491\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  21  - Train Loss:  1433.981  - Val Loss:  1436.2898\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  22  - Train Loss:  1347.0616  - Val Loss:  1350.9297\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  23  - Train Loss:  1274.4215  - Val Loss:  1296.7688\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  24  - Train Loss:  1217.5923  - Val Loss:  1235.0399\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  25  - Train Loss:  1176.2174  - Val Loss:  1204.0619\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  26  - Train Loss:  1072.3224  - Val Loss:  963.2734\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  27  - Train Loss:  844.1388  - Val Loss:  845.2282\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  28  - Train Loss:  753.8218  - Val Loss:  747.5536\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  29  - Train Loss:  676.4752  - Val Loss:  678.5104\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  30  - Train Loss:  618.9015  - Val Loss:  630.7735\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  31  - Train Loss:  575.207  - Val Loss:  586.7339\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  32  - Train Loss:  542.9136  - Val Loss:  566.185\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  33  - Train Loss:  520.5223  - Val Loss:  541.239\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  34  - Train Loss:  499.0517  - Val Loss:  519.1832\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  35  - Train Loss:  480.7108  - Val Loss:  489.2176\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  36  - Train Loss:  466.9938  - Val Loss:  490.0503\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  37  - Train Loss:  454.2147  - Val Loss:  473.0085\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  38  - Train Loss:  440.5589  - Val Loss:  456.646\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  39  - Train Loss:  428.5339  - Val Loss:  451.1684\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  40  - Train Loss:  424.4304  - Val Loss:  447.9969\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  41  - Train Loss:  418.2078  - Val Loss:  437.6243\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  42  - Train Loss:  412.8891  - Val Loss:  437.4589\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  43  - Train Loss:  411.3733  - Val Loss:  442.0149\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  44  - Train Loss:  406.6442  - Val Loss:  428.5068\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  45  - Train Loss:  401.5027  - Val Loss:  419.854\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  46  - Train Loss:  397.6688  - Val Loss:  413.4745\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  47  - Train Loss:  398.6834  - Val Loss:  426.1001\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  48  - Train Loss:  393.2975  - Val Loss:  410.132\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  49  - Train Loss:  394.2599  - Val Loss:  415.7321\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  50  - Train Loss:  389.5328  - Val Loss:  410.5107\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  51  - Train Loss:  391.3879  - Val Loss:  422.232\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  52  - Train Loss:  387.0503  - Val Loss:  410.1666\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  53  - Train Loss:  384.4063  - Val Loss:  416.2105\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  54  - Train Loss:  384.4014  - Val Loss:  416.6993\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  55  - Train Loss:  382.858  - Val Loss:  402.4853\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  56  - Train Loss:  379.833  - Val Loss:  411.3559\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  57  - Train Loss:  379.8659  - Val Loss:  402.3617\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  58  - Train Loss:  380.6561  - Val Loss:  409.5829\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  59  - Train Loss:  379.0757  - Val Loss:  401.3752\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  60  - Train Loss:  379.8556  - Val Loss:  404.5408\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  61  - Train Loss:  380.6948  - Val Loss:  402.87\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  62  - Train Loss:  377.8648  - Val Loss:  404.0665\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  63  - Train Loss:  376.7788  - Val Loss:  408.5388\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  64  - Train Loss:  378.158  - Val Loss:  388.8004\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  65  - Train Loss:  374.7087  - Val Loss:  400.4539\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  66  - Train Loss:  376.8074  - Val Loss:  395.8602\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  67  - Train Loss:  373.1085  - Val Loss:  401.8077\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  68  - Train Loss:  374.4136  - Val Loss:  387.4626\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  69  - Train Loss:  371.0309  - Val Loss:  382.2081\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  70  - Train Loss:  375.7414  - Val Loss:  393.0018\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 1 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  71  - Train Loss:  373.9092  - Val Loss:  401.8972\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 2 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  72  - Train Loss:  370.0823  - Val Loss:  405.7956\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 3 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  73  - Train Loss:  370.1062  - Val Loss:  398.6671\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 4 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  74  - Train Loss:  372.0011  - Val Loss:  402.9023\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 5 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  75  - Train Loss:  370.4659  - Val Loss:  392.0135\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 6 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  76  - Train Loss:  368.1894  - Val Loss:  393.8292\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 7 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  77  - Train Loss:  370.1626  - Val Loss:  395.0916\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 8 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  78  - Train Loss:  369.8164  - Val Loss:  396.8181\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 9 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  79  - Train Loss:  367.8508  - Val Loss:  394.5169\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 10 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  80  - Train Loss:  370.8539  - Val Loss:  389.2602\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch 00080: reducing learning rate of group 0 to 1.0000e-03.\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 11 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  81  - Train Loss:  364.7124  - Val Loss:  398.0676\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 12 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  82  - Train Loss:  358.928  - Val Loss:  394.1622\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 13 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  83  - Train Loss:  362.816  - Val Loss:  382.6392\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 14 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  84  - Train Loss:  361.6968  - Val Loss:  385.1964\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 15 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  85  - Train Loss:  359.6553  - Val Loss:  388.8541\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 16 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  86  - Train Loss:  359.573  - Val Loss:  382.3365\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 17 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  87  - Train Loss:  360.0162  - Val Loss:  391.9785\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 18 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  88  - Train Loss:  363.0304  - Val Loss:  386.8431\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 19 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m Epoch:  89  - Train Loss:  359.5103  - Val Loss:  388.7117\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping counter 20 of 20\n","\u001b[2m\u001b[36m(train_nn pid=424)\u001b[0m INFO: Early stopping\n"]},{"name":"stderr","output_type":"stream","text":["2023-05-07 09:15:34,660\tINFO timeout.py:54 -- Reached timeout of 600 seconds. Stopping all trials.\n","2023-05-07 09:15:34,768\tINFO tune.py:798 -- Total run time: 862.81 seconds (862.49 seconds for the tuning loop).\n"]}],"source":["# Time the round\n","start_time = time.time()\n","\n","result = flaml.tune.run(tune.with_parameters(train_nn),   # The training loop is wrapped inside function train_nn\n","                        config = config,                  # The HP search space dictionary\n","                        metric = \"loss\",                  # Metric to chose the best case. It is stored in tune.report within train_nn\n","                        mode = \"min\",                     # Lower is better\n","                        resources_per_trial = {\"cpu\": 4}, # Define usable hardware\n","                        local_dir = 'logs/',              # Set output directory\n","                        num_samples = num_samples,        # Search budget: maximum number of tests\n","                        time_budget_s = time_budget_s,    # Search budget: maximum time\n","                        use_ray=True)"]},{"cell_type":"code","execution_count":14,"id":"79cb045a","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:15:34.861809Z","iopub.status.busy":"2023-05-07T09:15:34.861393Z","iopub.status.idle":"2023-05-07T09:15:34.87021Z","shell.execute_reply":"2023-05-07T09:15:34.868444Z"},"papermill":{"duration":0.042846,"end_time":"2023-05-07T09:15:34.872936","exception":false,"start_time":"2023-05-07T09:15:34.83009","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trials = 4\n","Time = 869.8790180683136 seconds\n","Best trial config: {'layer1': 200, 'layer2': 200, 'layer3': 200, 'lr': 0.01, 'batch_size': 100, 'activation': 'ReLU', 'dropout_prob': 0}\n","Best trial final validation loss: 357.69793701171875\n"]}],"source":["# Print out outcome of the round: number of trials and time\n","print(f\"Number of trials = {len(result.trials)}\")\n","print(f\"Time = {time.time()-start_time} seconds\")\n","\n","# Extract the best case\n","best_trial = result.get_best_trial(\"loss\", \"min\", \"all\")\n","\n","# Give some info on the best result\n","print(\"Best trial config: {}\".format(best_trial.config))\n","print(\"Best trial final validation loss: {}\".format(\n","    best_trial.metric_analysis[\"loss\"][\"min\"]))"]},{"cell_type":"markdown","id":"91880d02","metadata":{"papermill":{"duration":0.033533,"end_time":"2023-05-07T09:15:34.935602","exception":false,"start_time":"2023-05-07T09:15:34.902069","status":"completed"},"tags":[]},"source":["# **Predictions on the test set and submission**"]},{"cell_type":"code","execution_count":15,"id":"0302998d","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-05-07T09:15:35.000247Z","iopub.status.busy":"2023-05-07T09:15:34.999042Z","iopub.status.idle":"2023-05-07T09:15:35.024869Z","shell.execute_reply":"2023-05-07T09:15:35.023131Z"},"papermill":{"duration":0.062263,"end_time":"2023-05-07T09:15:35.027853","exception":false,"start_time":"2023-05-07T09:15:34.96559","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Extract best hyperparameters and initialize a new model with these values\n","n_features = X_train.shape[1]\n","\n","best_hidden_dims = [best_trial.config[\"layer1\"], best_trial.config[\"layer2\"], best_trial.config[\"layer3\"]]\n","best_trained_model = get_model(n_features, best_hidden_dims, best_trial.config[\"activation\"], best_trial.config[\"dropout_prob\"])\n","\n","# From the saved checkpoint load the state of the best model for further predictions\n","checkpoint_value = getattr(best_trial.checkpoint, \"dir_or_data\", None) or best_trial.checkpoint.value\n","checkpoint_path = os.path.join(checkpoint_value, \"checkpoint\")\n","\n","model_state, optimizer_state = torch.load(checkpoint_path)\n","best_trained_model.load_state_dict(model_state)"]},{"cell_type":"code","execution_count":16,"id":"5b3378fb","metadata":{"execution":{"iopub.execute_input":"2023-05-07T09:15:35.094651Z","iopub.status.busy":"2023-05-07T09:15:35.093717Z","iopub.status.idle":"2023-05-07T09:15:35.262567Z","shell.execute_reply":"2023-05-07T09:15:35.261316Z"},"papermill":{"duration":0.203865,"end_time":"2023-05-07T09:15:35.265284","exception":false,"start_time":"2023-05-07T09:15:35.061419","status":"completed"},"tags":[]},"outputs":[],"source":["submission = pd.read_csv(r'../input/playground-series-s3e14/test.csv')\n","ids = submission.pop('id')\n","\n","# Apply scaling to the features of the test set\n","submission_proc = preproc_pipeline.transform(submission)  \n","\n","predictions = best_trained_model(torch.tensor(submission_proc).type(torch.float32))\n","\n","result = pd.DataFrame(torch.hstack((torch.tensor(ids.to_numpy().reshape((-1, 1))), predictions.detach())), columns=['id','yield'])\n","result['id'] = result['id'].astype(int)\n","result.to_csv('submission.csv',index =False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":916.864602,"end_time":"2023-05-07T09:15:40.416074","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-07T09:00:23.551472","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}