{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/eleonoraricci/learning-pipelines-productivity-reusability?scriptVersionId=128571613\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"62aab48c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.010027,"end_time":"2023-05-06T20:06:21.547575","exception":false,"start_time":"2023-05-06T20:06:21.537548","status":"completed"},"tags":[]},"source":["# **Leverage Pipelines to enhance reusability and increase productivity**"]},{"cell_type":"markdown","id":"fe20420f","metadata":{"papermill":{"duration":0.007743,"end_time":"2023-05-06T20:06:21.563779","exception":false,"start_time":"2023-05-06T20:06:21.556036","status":"completed"},"tags":[]},"source":["Recently I've been learning about **sklearn pipelines** and I wanted to apply this useful workflow to the Titanic competition, to streamline data handling and model training, in a way that can be **easily reused** with other datasets or to simplify model and feature engineering tests, and I thought it would be useful to share a walkthrough applied to this dataset.\n","\n","Here I don't dig deep into the feature engineering aspects, but focus on showcasing a general and reusable preprocessing workflow. \n","\n","Let's dig in! ðŸ’ª"]},{"cell_type":"code","execution_count":1,"id":"7442a7f6","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:21.582851Z","iopub.status.busy":"2023-05-06T20:06:21.581911Z","iopub.status.idle":"2023-05-06T20:06:22.911702Z","shell.execute_reply":"2023-05-06T20:06:22.91072Z"},"papermill":{"duration":1.342605,"end_time":"2023-05-06T20:06:22.91456","exception":false,"start_time":"2023-05-06T20:06:21.571955","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer\n","\n","seed = 17"]},{"cell_type":"code","execution_count":2,"id":"7d3dda83","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:22.933359Z","iopub.status.busy":"2023-05-06T20:06:22.932669Z","iopub.status.idle":"2023-05-06T20:06:22.964814Z","shell.execute_reply":"2023-05-06T20:06:22.963554Z"},"papermill":{"duration":0.044657,"end_time":"2023-05-06T20:06:22.967532","exception":false,"start_time":"2023-05-06T20:06:22.922875","status":"completed"},"tags":[]},"outputs":[],"source":["training_set = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n","submission_set = pd.read_csv(\"/kaggle/input/titanic/test.csv\")"]},{"cell_type":"markdown","id":"8e07e33e","metadata":{"papermill":{"duration":0.007944,"end_time":"2023-05-06T20:06:22.983922","exception":false,"start_time":"2023-05-06T20:06:22.975978","status":"completed"},"tags":[]},"source":["# **Check missing values**"]},{"cell_type":"code","execution_count":3,"id":"a46028fe","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.003313Z","iopub.status.busy":"2023-05-06T20:06:23.002825Z","iopub.status.idle":"2023-05-06T20:06:23.014942Z","shell.execute_reply":"2023-05-06T20:06:23.013943Z"},"papermill":{"duration":0.025057,"end_time":"2023-05-06T20:06:23.018044","exception":false,"start_time":"2023-05-06T20:06:22.992987","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing values in training set : \n","PassengerId      0\n","Survived         0\n","Pclass           0\n","Name             0\n","Sex              0\n","Age            177\n","SibSp            0\n","Parch            0\n","Ticket           0\n","Fare             0\n","Cabin          687\n","Embarked         2\n","dtype: int64\n","\n","\n","Missing values in submission set : \n","PassengerId      0\n","Pclass           0\n","Name             0\n","Sex              0\n","Age             86\n","SibSp            0\n","Parch            0\n","Ticket           0\n","Fare             1\n","Cabin          327\n","Embarked         0\n","dtype: int64\n","\n","\n"]}],"source":["#Check which columns have missing values\n","print(\"Missing values in training set : \")\n","print(training_set.isna().sum())\n","print(\"\\n\")\n","print(\"Missing values in submission set : \")\n","print(submission_set.isna().sum())\n","print(\"\\n\")"]},{"cell_type":"markdown","id":"b352211a","metadata":{"papermill":{"duration":0.008036,"end_time":"2023-05-06T20:06:23.034532","exception":false,"start_time":"2023-05-06T20:06:23.026496","status":"completed"},"tags":[]},"source":["# **Preprocessing steps**\n","\n","For starters, we can divide the preprocessing into several steps:\n","- Features to drop\n","- Features containing missing values\n","- Features to encode\n","- Features to scale/normalize"]},{"cell_type":"code","execution_count":4,"id":"8180f911","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.054827Z","iopub.status.busy":"2023-05-06T20:06:23.053593Z","iopub.status.idle":"2023-05-06T20:06:23.060461Z","shell.execute_reply":"2023-05-06T20:06:23.059203Z"},"papermill":{"duration":0.019176,"end_time":"2023-05-06T20:06:23.063054","exception":false,"start_time":"2023-05-06T20:06:23.043878","status":"completed"},"tags":[]},"outputs":[],"source":["# Listing the columns that we want to pre-process with each strategy. Here no missing values to handle.\n","fill_with_mean = []\n","fill_with_median = [\"Fare\"]\n","fill_with_most_frequent = []\n","fill_with_value, fill_value = [], None  #Neglecting \"Cabin\" and \"Embarked\" because we'll drop them in this example\n","\n","one_hot_encode = [\"Sex\"]\n","ordinal_encode = []\n","\n","columns_to_drop = [\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\", \"Embarked\", \"Age\", \"Parch\", \"SibSp\"]\n","\n","min_max_scale = []\n","standard_scale = []\n","to_normalize = []"]},{"cell_type":"markdown","id":"42bf0318","metadata":{"papermill":{"duration":0.008296,"end_time":"2023-05-06T20:06:23.080128","exception":false,"start_time":"2023-05-06T20:06:23.071832","status":"completed"},"tags":[]},"source":["# **Dropping DataFrame columns**\n","We can accomplish this by [creating a custom column transformer](https://www.andrewvillazon.com/custom-scikit-learn-transformers/) to add to the pipeline, to remove feature we don't want to use.\n","By inheriting from *BaseEstimator* and *TransformerMixin* and implementing the *fit* and *transform* methods, we achieve the required functionality and we can then use it inside the pipelines."]},{"cell_type":"code","execution_count":5,"id":"57661390","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.098639Z","iopub.status.busy":"2023-05-06T20:06:23.098174Z","iopub.status.idle":"2023-05-06T20:06:23.105609Z","shell.execute_reply":"2023-05-06T20:06:23.104302Z"},"papermill":{"duration":0.019711,"end_time":"2023-05-06T20:06:23.108147","exception":false,"start_time":"2023-05-06T20:06:23.088436","status":"completed"},"tags":[]},"outputs":[],"source":["class ColumnsDropper(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns=None):\n","        super(ColumnsDropper, self).__init__()\n","        self.columns=columns\n","\n","    def transform(self,X, y=None):\n","        return X.drop(self.columns,axis=1)\n","\n","    def fit(self, X, y=None):\n","        return self"]},{"cell_type":"markdown","id":"74ffa9d2","metadata":{"papermill":{"duration":0.008079,"end_time":"2023-05-06T20:06:23.124794","exception":false,"start_time":"2023-05-06T20:06:23.116715","status":"completed"},"tags":[]},"source":["# **Imputing missing values**\n","\n","We can define various imputers to **handle missing values** according to a specified strategy, using the sklearn SimpleImputer. Different imputer strategies can be applied to different columns. \"*Mean*\" and \"*median*\" strategies can only be applied to numerical columns, \"*most_frequent*\" and \"*constant*\" can be applied to both strings and numbers.\n","\n","We can feed **empty lists** to our Pipeline, therefore we can **keep the implementation general** and have the code ready for reuse in more complex cases by defining a **ColumnTransformer** that applies all imputing strategies to the specified lists of columns. It is important to specify the *remainder='passthrough'* argument, otherwise the default behaviour is to drop the columns that were not transformed by any transformer.\n","\n","**[SimpleImputer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)**\n","\n","**[ColumnTransformer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)**"]},{"cell_type":"code","execution_count":6,"id":"83e4a26c","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.14403Z","iopub.status.busy":"2023-05-06T20:06:23.14286Z","iopub.status.idle":"2023-05-06T20:06:23.149801Z","shell.execute_reply":"2023-05-06T20:06:23.148544Z"},"papermill":{"duration":0.019433,"end_time":"2023-05-06T20:06:23.152484","exception":false,"start_time":"2023-05-06T20:06:23.133051","status":"completed"},"tags":[]},"outputs":[],"source":["col_transformers=[\n","    ('mean', SimpleImputer(strategy='mean'), fill_with_mean),\n","    ('median', SimpleImputer(strategy='median'), fill_with_median),\n","    ('most_frequent', SimpleImputer(strategy='most_frequent'), fill_with_most_frequent),\n","    ('value', SimpleImputer(strategy='constant', fill_value = fill_value), fill_with_value),        \n","]"]},{"cell_type":"markdown","id":"56e90f20","metadata":{"papermill":{"duration":0.007841,"end_time":"2023-05-06T20:06:23.168557","exception":false,"start_time":"2023-05-06T20:06:23.160716","status":"completed"},"tags":[]},"source":["# **Encoding categorical variables**\n","We may need to use an encoder to transform labeled categories into a numerical input, either with **Ordinal Encoding** or **One-Hot encoding**.\n","\n","|City | Ordinal Encoding | One-Hot Encoding|\n","|----|------|------|\n","|Tokyo| 0 | [1,0,0] |\n","|Madrid| 1 |[0,1,0] |\n","|Tunisi| 2 |[0,0,1] |\n","\n","Or maybe we have a categorical variable expressed numerically and we want to transform it to a One-Hot encoded one. For example, we could have a variable \"Season\" taking values 0, 1, 2, 3, which are numerical, but they do not really contain a numerical relationship between the values (it is not meaningful to consider \"autumn\" > \" winter\" or vice versa), therefore we might prefer to one-hot encode it instead.\n","\n","Documentation for **[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)**, **[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)**, and **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)**. "]},{"cell_type":"code","execution_count":7,"id":"e495363b","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.187521Z","iopub.status.busy":"2023-05-06T20:06:23.186347Z","iopub.status.idle":"2023-05-06T20:06:23.191923Z","shell.execute_reply":"2023-05-06T20:06:23.19108Z"},"papermill":{"duration":0.017408,"end_time":"2023-05-06T20:06:23.194201","exception":false,"start_time":"2023-05-06T20:06:23.176793","status":"completed"},"tags":[]},"outputs":[],"source":["# We append the Encoders to the column transformers list\n","col_transformers.append(('one_hot', OneHotEncoder(), one_hot_encode))\n","col_transformers.append(('ordinal', OrdinalEncoder(), ordinal_encode))"]},{"cell_type":"markdown","id":"9ae58d22","metadata":{"papermill":{"duration":0.00878,"end_time":"2023-05-06T20:06:23.211405","exception":false,"start_time":"2023-05-06T20:06:23.202625","status":"completed"},"tags":[]},"source":["Pipelines handle tranformation of the input data, not of the target, therefore these need to be handled separately. In this case we need to obtain an Ordinal Encoding of the labels, thus we can apply LabelEncoder."]},{"cell_type":"code","execution_count":8,"id":"c672c92d","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.230545Z","iopub.status.busy":"2023-05-06T20:06:23.230108Z","iopub.status.idle":"2023-05-06T20:06:23.235143Z","shell.execute_reply":"2023-05-06T20:06:23.234007Z"},"papermill":{"duration":0.017689,"end_time":"2023-05-06T20:06:23.237462","exception":false,"start_time":"2023-05-06T20:06:23.219773","status":"completed"},"tags":[]},"outputs":[],"source":["# This is an Ordinal transformer and it should be used to encode target values, i.e. y, and not the input X.\n","label_encoder = LabelEncoder()"]},{"cell_type":"markdown","id":"c4f02c91","metadata":{"papermill":{"duration":0.007881,"end_time":"2023-05-06T20:06:23.253747","exception":false,"start_time":"2023-05-06T20:06:23.245866","status":"completed"},"tags":[]},"source":["# **Scaling/Standardizing/Normalizing**\n","\n","Many models benefit in terms of accuracy and robustness when features are on a relatively similar scale and close to normally distributed. **MinMaxScaler**, **StandardScaler**, and **Normalizer** are scikit-learn methods to preprocess data in this way, with different use cases, which are discussed, for example, in this [blog post](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)."]},{"cell_type":"code","execution_count":9,"id":"aa91bc48","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.27224Z","iopub.status.busy":"2023-05-06T20:06:23.271793Z","iopub.status.idle":"2023-05-06T20:06:23.277449Z","shell.execute_reply":"2023-05-06T20:06:23.276449Z"},"papermill":{"duration":0.017655,"end_time":"2023-05-06T20:06:23.279595","exception":false,"start_time":"2023-05-06T20:06:23.26194","status":"completed"},"tags":[]},"outputs":[],"source":["col_transformers.append(('min_max', MinMaxScaler(), min_max_scale))\n","col_transformers.append(('std', StandardScaler(), standard_scale))\n","col_transformers.append(('norm', Normalizer(), to_normalize))"]},{"cell_type":"markdown","id":"23793b18","metadata":{"papermill":{"duration":0.00785,"end_time":"2023-05-06T20:06:23.295706","exception":false,"start_time":"2023-05-06T20:06:23.287856","status":"completed"},"tags":[]},"source":["# **Collecting all the pieces**\n","With all the preprocessing steps listed, we can create a ColumnTransformer, that will take case of the preprocessing.\n","\n","Documentation for **[ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)**."]},{"cell_type":"code","execution_count":10,"id":"5fd607c6","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.314307Z","iopub.status.busy":"2023-05-06T20:06:23.313824Z","iopub.status.idle":"2023-05-06T20:06:23.319307Z","shell.execute_reply":"2023-05-06T20:06:23.318191Z"},"papermill":{"duration":0.017902,"end_time":"2023-05-06T20:06:23.321839","exception":false,"start_time":"2023-05-06T20:06:23.303937","status":"completed"},"tags":[]},"outputs":[],"source":["preprocessor = ColumnTransformer(\n","    transformers=col_transformers,      \n","    remainder='passthrough')"]},{"cell_type":"markdown","id":"2511fabc","metadata":{"papermill":{"duration":0.00807,"end_time":"2023-05-06T20:06:23.33857","exception":false,"start_time":"2023-05-06T20:06:23.3305","status":"completed"},"tags":[]},"source":["# **Pipeline**\n","The **Pipeline** allows stacking various operations to perform on the data *in the specified sequence*, concluding with an estimator. A difference between a **Pipeline** and a **ColumnTransformer**, is that in the Pipeline all the steps specified are **applied to the same data**, whereas in the ColumnTransformer we can **specify different columns for each action to perform**. \n","\n","Intermediate steps of the pipeline must be *transforms*, that is, they must implement *fit* and *transform* methods. The final estimator only needs to implement *fit*. \n","\n","In the example in the comment below one has (1) defined a single transformation for the numerical features (the SimpleImputer), (2) created a Pipeline of transformations to be performed in sequence for the categorical variables (fill in missing values and then OneHotEncoding). (3) All defined column transformations are then grouped into a ColumnTransformer.\n","\n","Documentation for **[Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)**."]},{"cell_type":"code","execution_count":11,"id":"d6d06b6e","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.35699Z","iopub.status.busy":"2023-05-06T20:06:23.356591Z","iopub.status.idle":"2023-05-06T20:06:23.361119Z","shell.execute_reply":"2023-05-06T20:06:23.359918Z"},"papermill":{"duration":0.01702,"end_time":"2023-05-06T20:06:23.363832","exception":false,"start_time":"2023-05-06T20:06:23.346812","status":"completed"},"tags":[]},"outputs":[],"source":["######   EXAMPLE   ######\n","## Preprocessing for numerical data\n","#numerical_transformer = SimpleImputer(strategy='constant')\n","\n","## Preprocessing for categorical data\n","#categorical_transformer = Pipeline(steps=[\n","#    ('imputer', SimpleImputer(strategy='most_frequent')),\n","#    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","#])\n","\n","# Bundle preprocessing for numerical and categorical data\n","#preprocessor = ColumnTransformer(\n","#    transformers=[\n","#        ('num', numerical_transformer, numerical_cols),\n","#        ('cat', categorical_transformer, categorical_cols)\n","#    ])"]},{"cell_type":"markdown","id":"749f2112","metadata":{"papermill":{"duration":0.008027,"end_time":"2023-05-06T20:06:23.380269","exception":false,"start_time":"2023-05-06T20:06:23.372242","status":"completed"},"tags":[]},"source":["### **>>Insert Model Here<<**\n","At this point we need **initialize the model**, to concatenate data processing and model fitting in the subsequent cell."]},{"cell_type":"code","execution_count":12,"id":"86983e7f","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.399313Z","iopub.status.busy":"2023-05-06T20:06:23.398171Z","iopub.status.idle":"2023-05-06T20:06:23.569822Z","shell.execute_reply":"2023-05-06T20:06:23.568524Z"},"papermill":{"duration":0.184399,"end_time":"2023-05-06T20:06:23.572828","exception":false,"start_time":"2023-05-06T20:06:23.388429","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.ensemble._forest import RandomForestClassifier\n","\n","model = RandomForestClassifier(criterion='gini',\n","                               n_estimators=1750,\n","                               max_depth=7,\n","                               min_samples_split=6,\n","                               min_samples_leaf=6,\n","                               max_features='auto',\n","                               oob_score=True,\n","                               random_state=seed,\n","                               n_jobs=-1,\n","                               verbose=1) "]},{"cell_type":"markdown","id":"fa895be5","metadata":{"papermill":{"duration":0.007884,"end_time":"2023-05-06T20:06:23.589019","exception":false,"start_time":"2023-05-06T20:06:23.581135","status":"completed"},"tags":[]},"source":["Finally we can **concatenate the preprocessing and modelling in the final Pipeline**. \n","\n","Yes, **pipelines can be nested**! Here the *dropper* is a Pipeline itself, and the *preprocessor* is a ColumnTransformer that in general could contain Pipelines as well. This nested structure creates a hierarchy which controls the sequence of the execution."]},{"cell_type":"code","execution_count":13,"id":"5917a259","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.608147Z","iopub.status.busy":"2023-05-06T20:06:23.607432Z","iopub.status.idle":"2023-05-06T20:06:23.613651Z","shell.execute_reply":"2023-05-06T20:06:23.612562Z"},"papermill":{"duration":0.01884,"end_time":"2023-05-06T20:06:23.616201","exception":false,"start_time":"2023-05-06T20:06:23.597361","status":"completed"},"tags":[]},"outputs":[],"source":["# Bundle preprocessing and modeling code in a pipeline\n","my_pipeline = Pipeline(steps=[('dropper', ColumnsDropper(columns = columns_to_drop)),\n","                              ('preprocessor', preprocessor),\n","                              ('model', model) ]) "]},{"cell_type":"markdown","id":"efccb4be","metadata":{"papermill":{"duration":0.008148,"end_time":"2023-05-06T20:06:23.632685","exception":false,"start_time":"2023-05-06T20:06:23.624537","status":"completed"},"tags":[]},"source":["# **Splitting the data and training the model**"]},{"cell_type":"code","execution_count":14,"id":"fe49b2c4","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:23.651118Z","iopub.status.busy":"2023-05-06T20:06:23.650691Z","iopub.status.idle":"2023-05-06T20:06:23.669146Z","shell.execute_reply":"2023-05-06T20:06:23.668174Z"},"papermill":{"duration":0.030803,"end_time":"2023-05-06T20:06:23.671754","exception":false,"start_time":"2023-05-06T20:06:23.640951","status":"completed"},"tags":[]},"outputs":[],"source":["X = training_set.copy()\n","# We remove the target column from the training data... \n","y = X.pop(\"Survived\")\n","# ...and we split it in train and test sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=seed)"]},{"cell_type":"markdown","id":"89e25bdd","metadata":{"papermill":{"duration":0.007886,"end_time":"2023-05-06T20:06:23.688132","exception":false,"start_time":"2023-05-06T20:06:23.680246","status":"completed"},"tags":[]},"source":["The next line applies the pipeline to the train set: preprocessing and training the model."]},{"cell_type":"code","execution_count":15,"id":"c3341a1f","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-05-06T20:06:23.707044Z","iopub.status.busy":"2023-05-06T20:06:23.706376Z","iopub.status.idle":"2023-05-06T20:06:28.54311Z","shell.execute_reply":"2023-05-06T20:06:28.541836Z"},"papermill":{"duration":4.849321,"end_time":"2023-05-06T20:06:28.545641","exception":false,"start_time":"2023-05-06T20:06:23.69632","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.4s\n","[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.8s\n","[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.5s\n","[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    2.3s\n","[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    3.3s finished\n"]},{"data":{"text/plain":["Pipeline(steps=[('dropper',\n","                 ColumnsDropper(columns=['Name', 'Ticket', 'Cabin',\n","                                         'PassengerId', 'Embarked', 'Age',\n","                                         'Parch', 'SibSp'])),\n","                ('preprocessor',\n","                 ColumnTransformer(remainder='passthrough',\n","                                   transformers=[('mean', SimpleImputer(), []),\n","                                                 ('median',\n","                                                  SimpleImputer(strategy='median'),\n","                                                  ['Fare']),\n","                                                 ('most_frequent',\n","                                                  SimpleImputer(strategy='most_frequent'),\n","                                                  []),\n","                                                 ('value',\n","                                                  SimpleImputer(strategy='constant'),\n","                                                  []),\n","                                                 ('one_hot', OneHotEncoder(),\n","                                                  ['Sex']),\n","                                                 ('ordinal', OrdinalEncoder(),\n","                                                  []),\n","                                                 ('min_max', MinMaxScaler(),\n","                                                  []),\n","                                                 ('std', StandardScaler(), []),\n","                                                 ('norm', Normalizer(), [])])),\n","                ('model',\n","                 RandomForestClassifier(max_depth=7, min_samples_leaf=6,\n","                                        min_samples_split=6, n_estimators=1750,\n","                                        n_jobs=-1, oob_score=True,\n","                                        random_state=17, verbose=1))])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["my_pipeline.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"4bc03f0d","metadata":{"papermill":{"duration":0.008433,"end_time":"2023-05-06T20:06:28.562751","exception":false,"start_time":"2023-05-06T20:06:28.554318","status":"completed"},"tags":[]},"source":["# **Evaluate the model predictions**\n","Now with one line we can apply the same pipeline to the test data and get predictions from the trained model."]},{"cell_type":"code","execution_count":16,"id":"366b23f7","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:28.581965Z","iopub.status.busy":"2023-05-06T20:06:28.581555Z","iopub.status.idle":"2023-05-06T20:06:29.099554Z","shell.execute_reply":"2023-05-06T20:06:29.097692Z"},"papermill":{"duration":0.53116,"end_time":"2023-05-06T20:06:29.102514","exception":false,"start_time":"2023-05-06T20:06:28.571354","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n","[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n","[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.3s\n","[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.4s finished\n"]}],"source":["# Preprocessing of validation data, get predictions\n","preds = my_pipeline.predict(X_test)"]},{"cell_type":"code","execution_count":17,"id":"64499201","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:29.122727Z","iopub.status.busy":"2023-05-06T20:06:29.122328Z","iopub.status.idle":"2023-05-06T20:06:29.129636Z","shell.execute_reply":"2023-05-06T20:06:29.128233Z"},"papermill":{"duration":0.020759,"end_time":"2023-05-06T20:06:29.13243","exception":false,"start_time":"2023-05-06T20:06:29.111671","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8156424581005587\n"]}],"source":["# This competition uses the accuracy_score as a metric\n","from sklearn.metrics import accuracy_score\n","\n","# Evaluate the model\n","score = accuracy_score(y_test, preds)\n","print('Accuracy:', score)"]},{"cell_type":"markdown","id":"47989b64","metadata":{"papermill":{"duration":0.008946,"end_time":"2023-05-06T20:06:29.151251","exception":false,"start_time":"2023-05-06T20:06:29.142305","status":"completed"},"tags":[]},"source":["This concludes the demonstration of the Pipeline implementation. To go forward one could leverage the piepline to create a workflow to check different models, different hyperparameters, or different preprocessing strategies... all while keeping the code modular, readable, and resusable!"]},{"cell_type":"markdown","id":"d95805fb","metadata":{"papermill":{"duration":0.008529,"end_time":"2023-05-06T20:06:29.168805","exception":false,"start_time":"2023-05-06T20:06:29.160276","status":"completed"},"tags":[]},"source":["# **Submission**\n","After we have completed all preliminary analysis and chosen the best modelling approach, we can retrain the model using the whole training data to further increase its accuracy (in this final stage the test set is actually the one used to score the submission on the leaderboard!)."]},{"cell_type":"code","execution_count":18,"id":"78f86b2d","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-05-06T20:06:29.188931Z","iopub.status.busy":"2023-05-06T20:06:29.188504Z","iopub.status.idle":"2023-05-06T20:06:34.596618Z","shell.execute_reply":"2023-05-06T20:06:34.594424Z"},"papermill":{"duration":5.421063,"end_time":"2023-05-06T20:06:34.599022","exception":false,"start_time":"2023-05-06T20:06:29.177959","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.4s\n","[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.8s\n","[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.5s\n","[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:    2.3s\n","[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    3.3s finished\n","[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n","[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n","[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n","[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:    0.4s\n","[Parallel(n_jobs=4)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n"]}],"source":["my_pipeline.fit(X, y)\n","X_sub = submission_set.copy()\n","results = my_pipeline.predict(X_sub)"]},{"cell_type":"code","execution_count":19,"id":"180b7e11","metadata":{"execution":{"iopub.execute_input":"2023-05-06T20:06:34.620847Z","iopub.status.busy":"2023-05-06T20:06:34.620436Z","iopub.status.idle":"2023-05-06T20:06:34.632785Z","shell.execute_reply":"2023-05-06T20:06:34.631813Z"},"papermill":{"duration":0.025961,"end_time":"2023-05-06T20:06:34.635368","exception":false,"start_time":"2023-05-06T20:06:34.609407","status":"completed"},"tags":[]},"outputs":[],"source":["submission_df = pd.DataFrame({'PassengerId': submission_set['PassengerId'], 'Survived': results})\n","submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","id":"d71b6a2b","metadata":{"papermill":{"duration":0.00931,"end_time":"2023-05-06T20:06:34.65427","exception":false,"start_time":"2023-05-06T20:06:34.64496","status":"completed"},"tags":[]},"source":["Since I started using Pipelines I have greatly reduced the time spent writing boilerplate code and can concentrate more on the more creative aspects of ML modelling. ðŸŽ¨ I hope this walkthrough was useful and that others can benefit from implementing Pipelins as well!! ðŸ”¥ \n","\n","Happy modelling! :D"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":25.772442,"end_time":"2023-05-06T20:06:35.487475","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-06T20:06:09.715033","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}